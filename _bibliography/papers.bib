@misc{foroutan2025conlidsupervisedcontrastivelearning,
  title={ConLID: Supervised Contrastive Learning for Low-Resource Language Identification},
  author={Foroutan*, Negar and Saydaliev*, Jakhongir and Kim, Ye Eun and Bosselut, Antoine},
  journal={European Chapter of the Association for Computational Linguistics (EACL)},
  award={Winner of WMDQS Shared Task #2 at COLM 2025},
  year={2026},
  code={https://github.com/epfl-nlp/ConLID},
  pdf={https://arxiv.org/pdf/2506.15304},
  abstract = {Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, lowresource languages – often limited to single domain data, such as the Bible – continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. Through an extensive analysis, we show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2%, demonstrating its effectiveness in enhancing LID models.},
  preview = {conlid_figure.png},
  selected={true},
  other_contrib={false},
}

@article{karch2025llmagentsinteractiveexploration,
  title={LLM Agents for Interactive Exploration of Historical Cadastre Data: Framework and Application to Venice},
  DOI={10.1017/chr.2025.10014},
  journal={Computational Humanities Research (CHR)},
  author={Karch*, Tristan and Saydaliev*, Jakhongir and Di Lenardo, Isabella and Kaplan, Frédéric},
  code={https://github.com/dhlab-epfl/venice-agents},
  pdf={https://arxiv.org/pdf/2505.17148},
  abstract={Cadastral data reveal key information about the historical organization of cities but are often non-standardized due to diverse formats and human annotations, complicating large-scale analysis. We explore as a case study Venice's urban history during the critical period from 1740 to 1808, capturing the transition following the fall of the ancient Republic and the Ancien Régime. This era's complex cadastral data, marked by its volume and lack of uniform structure, presents unique challenges that our approach adeptly navigates, enabling us to generate spatial queries that bridge past and present urban landscapes. We present a text-to-programs framework that leverages Large Language Models (LLMs) to process natural language queries as executable code for analyzing historical cadastral records. Our methodology implements two complementary techniques: a SQL agent for handling structured queries about specific cadastral information, and a coding agent for complex analytical operations requiring custom data manipulation. We propose a taxonomy that classifies historical research questions based on their complexity and analytical requirements, mapping them to the most appropriate technical approach. This framework is supported by an investigation into the execution consistency of the system, alongside a qualitative analysis of the answers it produces. By ensuring interpretability and minimizing hallucination through verifiable program outputs, we demonstrate the system's effectiveness in reconstructing past population information, property features, and spatiotemporal comparisons in Venice.},
  preview={venice_figure.png},
  year={2025},
  selected={true},
  other_contrib={false}
}

@misc{hernándezcano2025apertusdemocratizingopencompliant,
  title={Apertus: Democratizing Open and Compliant LLMs for Global Language Environments}, 
  author={Apertus Team},
  journal={Submitted to Association for Computational Linguistics (ACL)},
  year={2026},
  code={https://huggingface.co/collections/swiss-ai/apertus-llm-68b699e65415c231ace3b059},
  pdf={https://arxiv.org/pdf/2509.14233},
  abstract={We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting this http URL exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.},
  preview={apertus.png},
  selected={true},
  other_contrib={false},
  note={Contributed to the pre-training data through my ConLID project},
}

@inproceedings{romanou2025include,
  title={INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge},
  author={Angelika Romanou and Negar Foroutan and Anna Sotnikova and Sree Harsha Nelaturu and Shivalika Singh and Rishabh Maheshwary and Micol Altomare and Zeming Chen and Mohamed A. Haggag and Snegha A and Alfonso Amayuelas and Azril Hafizi Amirudin and Danylo Boiko and Michael Chang and Jenny Chim and Gal Cohen and Aditya Kumar Dalmia and Abraham Diress and Sharad Duwal and Daniil Dzenhaliou and Daniel Fernando Erazo Florez and Fabian Farestam and Joseph Marvin Imperial and Shayekh Bin Islam and Perttu Isotalo and Maral Jabbarishiviari and B{\"o}rje F. Karlsson and Eldar Khalilov and Christopher Klamm and Fajri Koto and Dominik Krzemi{\'n}ski and Gabriel Adriano de Melo and Syrielle Montariol and Yiyang Nan and Joel Niklaus and Jekaterina Novikova and Johan Samir Obando Ceron and Debjit Paul and Esther Ploeger and Jebish Purbey and Swati Rajwal and Selvan Sunitha Ravi and Sara Rydell and Roshan Santhosh and Drishti Sharma and Marjana Prifti Skenduli and Arshia Soltani Moakhar and Bardia soltani moakhar and Ayush Kumar Tarun and Azmine Toushik Wasi and Thenuka Ovin Weerasinghe and Serhan Yilmaz and Mike Zhang and Imanol Schlag and Marzieh Fadaee and Sara Hooker and Antoine Bosselut},
  booktitle={International Conference on Learning Representations (ICLR)},
  award={Spotlight Paper (Top 5% of Papers)},
  year={2025},
  pdf={https://openreview.net/pdf?id=k3gCieTXeY},
  dataset={https://huggingface.co/datasets/CohereLabs/include-base-44},
  website={https://nlp.epfl.ch/include/},
  abstract={The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e., multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.},
  preview={include.png},
  selected={true},
  other_contrib={true},
  note={Contributed to collecting the Uzbek dataset},
}
