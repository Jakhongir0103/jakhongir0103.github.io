<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Mixed-modal Reasoning | Jakhongir Saydaliev </title> <meta name="author" content="Jakhongir Saydaliev"> <meta name="description" content="Trained 3 paradigms of visual reasoning using GRPO"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile.png?9eec2f327f2e2cf3da5d0adcec110cea"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jakhongir0103.github.io/projects/3_project/"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jakhongir</span> Saydaliev </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">cv </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Mixed-modal Reasoning</h1> <p class="post-description">Trained 3 paradigms of visual reasoning using GRPO</p> </header> <article> <div class="links" style="margin-bottom: 2rem;"> <a href="https://github.com/Jakhongir0103/multimodal_cot/blob/main/pdf/technical_report.pdf" class="btn btn-primary btn-sm" role="button" target="_blank" style="background-color: white !important; border: 1px solid black !important; color: black !important; padding: 8px 16px; border-radius: 4px; text-decoration: none; display: inline-block; margin-right: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" rel="external nofollow noopener"> <i class="fas fa-file-pdf"></i> Technical Report </a> <a href="https://github.com/Jakhongir0103/multimodal_cot" class="btn btn-primary btn-sm" role="button" target="_blank" style="background-color: white !important; border: 1px solid black !important; color: black !important; padding: 8px 16px; border-radius: 4px; text-decoration: none; display: inline-block; margin-right: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" rel="external nofollow noopener"> <i class="fa-brands fa-github gh-icon"></i> Code </a> </div> <p>Chain-of-thought reasoning has transformed how large language models approach complex problems by generating explicit intermediate steps. As vision-language models have matured, a fundamental question emerges: should reasoning chains remain purely textual, or can visual representations enhance the thinking process?</p> <p>This project investigates three paradigms for multimodal chain-of-thought reasoning:</p> <ul> <li> <strong>Multimodal-to-Multimodal</strong>: takes image and text as input and generates reasoning chains that interleave visual and textual thoughts</li> <li> <strong>Text-to-Multimodal</strong>: starts with text alone but produces multimodal reasoning chains including generated images</li> <li> <strong>Multimodal-to-Text</strong>: processes image and text inputs but constrains reasoning to textual chains enriched with visual grounding elements like bounding boxes</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/syrielle_diagram-480.webp 480w,/assets/img/projects/syrielle_diagram-800.webp 800w,/assets/img/projects/syrielle_diagram-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/syrielle_diagram.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Three paradigms of multimodal reasoning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Three variations of multimodal reasoning paradigms, differing in how visual and textual information flows through the reasoning process. </div> <p>A key challenge is the scarcity of datasets with ground-truth reasoning chains. To address this, we employed Group Relative Policy Optimization (GRPO$^{[1]}$), a reinforcement learning technique that learns reasoning strategies from input-output pairs alone, guided by paradigm-specific reward functions.</p> <h2 id="methodology">Methodology</h2> <p><strong>Multimodal-to-Multimodal Reasoning</strong></p> <p>I used <a href="https://huggingface.co/GAIR/Anole-7b-v0.1" rel="external nofollow noopener" target="_blank">Anole7B</a>, built on the Chameleon$^{[2]}$ architecture, which processes both image and text tokens through a unified transformer. I fine-tuned the model using LoRA$^{[3]}$ on <a href="https://huggingface.co/datasets/declare-lab/PuzzleVQA" rel="external nofollow noopener" target="_blank">PuzzleVQA</a>, a dataset of synthetically generated puzzles involving abstract patterns with colors, numbers, sizes, and shapes. I trained both multimodal and textual-only versions to compare performance across 20 different puzzle types, with 5 used for training (in-domain) and 15 held out (out-of-domain).</p> <p><strong>Text-to-Multimodal Reasoning</strong></p> <p>For this paradigm, I trained <a href="https://huggingface.co/AILab-CVC/seed-llama-8b-sft" rel="external nofollow noopener" target="_blank">SEED-LLaMA-8B</a> using GRPO on the <a href="https://huggingface.co/datasets/tasksource/ReSQ" rel="external nofollow noopener" target="_blank">ReSQ</a> dataset, containing about 1,000 questions involving spatial reasoning about described scenes. The model learned through four reward functions: formatting compliance, accuracy, image-text alignment (cosine similarity between descriptions and generated images), and a penalty for excessive image generation. I compared against a textual GRPO baseline and a strong baseline using DALL-E 3 for visualization and GPT-4o for reasoning.</p> <p><strong>Multimodal-to-Text Reasoning</strong></p> <p>This paradigm used a two-stage pipeline with <a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct" rel="external nofollow noopener" target="_blank">Qwen2.5-VL-7B</a>. First, supervised fine-tuning on reasoning chains with interleaved bounding boxes warmed up the model. Second, GRPO training optimized three rewards: accuracy, format compliance, and IoU scores measuring bounding box quality. I trained on <a href="https://huggingface.co/datasets/EPFL-DrivingVQA/DrivingVQA" rel="external nofollow noopener" target="_blank">DrivingVQA</a> and <a href="https://huggingface.co/datasets/HuggingFaceM4/A-OKVQA" rel="external nofollow noopener" target="_blank">A-OKVQA</a>, totaling about 20,000 examples with ground-truth reasoning traces containing bounding boxes.</p> <div class="row justify-content-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/syrielle_grpo-480.webp 480w,/assets/img/projects/syrielle_grpo-800.webp 800w,/assets/img/projects/syrielle_grpo-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/syrielle_grpo.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GRPO training pipeline" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mt-2"> Multimodal-to-Text reasoning: Two-stage training pipeline of with SFT warmup followed by GRPO optimization. </div> <h2 id="results">Results</h2> <p><strong>Images Hurt Performance</strong></p> <p>Generating images during reasoning consistently degraded performance across both paradigms that produced visual outputs.</p> <p><em>Multimodal-to-Multimodal</em>: Textual-only reasoning achieved a higher perofrmance over multimodal reasoning on PuzzleVQA, with the gap consistent across both in-domain and out-of-domain puzzles.</p> <table data-toggle="table" class="table table-bordered table-hover text-center align-middle"> <thead> <tr> <th># Puzzle Types</th> <th>Domain Type</th> <th>Multimodal</th> <th>Textual</th> </tr> </thead> <tbody> <tr> <td>5</td> <td>ID</td> <td>0.574</td> <td>0.744</td> </tr> <tr> <td>15</td> <td>OOD</td> <td>0.123</td> <td>0.195</td> </tr> </tbody> </table> <div class="caption text-center mt-2"> Accuracy of Anole7B fine-tuned on PuzzleVQA. </div> <p>Below are examples of consistent and inconsistent interleaved image generations:</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/syrielle_t2m_example1-480.webp 480w,/assets/img/projects/syrielle_t2m_example1-800.webp 800w,/assets/img/projects/syrielle_t2m_example1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/syrielle_t2m_example1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/syrielle_t2m_example2-480.webp 480w,/assets/img/projects/syrielle_t2m_example2-800.webp 800w,/assets/img/projects/syrielle_t2m_example2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/syrielle_t2m_example2.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> </swiper-container> <p><em>Text-to-Multimodal</em>: The best textual approach reached 0.450 Pass@1 on ReSQ, while the best multimodal variant achieved only 0.411. Even the DALL-E 3 + GPT-4o baseline showed textual reasoning (0.761) outperforming multimodal reasoning (0.695).</p> <p>Including image-based rewards during GRPO training further degraded performance, suggesting the optimization struggled to effectively leverage visual information.</p> <p><strong>Visual Grounding Helps</strong></p> <p>The Multimodal-to-Text paradigm demonstrated clear benefits from visual grounding. GRPO training improved F1 scores on both A-OKVQA (86.78 → 88.12) and DrivingVQA (51.86 → 53.60).</p> <table data-toggle="table" class="table table-bordered table-hover text-center align-middle"> <thead> <tr> <th>Method</th> <th>DrivingVQA (F1)</th> <th>A-OKVQA (F1)</th> </tr> </thead> <tbody> <tr> <td><b>SFT-warmup</b></td> <td>51.86</td> <td>86.78</td> </tr> <tr> <td><b>GRPO</b></td> <td>53.6</td> <td>88.12</td> </tr> </tbody> </table> <div class="caption text-center mt-2"> F1 score of Qwen2.5-VL-7B trained with 2-stage pipeline on 2 datasets. </div> <p>Reward ablations revealed that IoU-based bounding box rewards particularly benefited out-of-domain performance, while format rewards had negligible impact since the SFT warmup already taught proper formatting. The most effective configuration combined accuracy and IoU rewards.</p> <p>Below is an example of the model’s output:</p> <div class="row justify-content-center"> <div class="col-10 col-md-8 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/syrielle_multimodel2text_example-480.webp 480w,/assets/img/projects/syrielle_multimodel2text_example-800.webp 800w,/assets/img/projects/syrielle_multimodel2text_example-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/syrielle_multimodel2text_example.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multimodal-to-text reasoning with bounding boxes" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mt-2"> Example of Multimodal-to-Text reasoning with interleaved bounding boxes grounding attention on specific visual regions. </div> <p><strong>Scaling to 120K Examples</strong></p> <p>I scaled the Multimodal-to-Text approach to approximately 120,000 samples across 10 datasets spanning fine-grained understanding, relation reasoning, text comprehension, general VQA, and chart interpretation. Training <a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct" rel="external nofollow noopener" target="_blank">Qwen2.5-VL-3B</a> on this corpus revealed two key insights:</p> <ul> <li>GRPO training consistently outperformed both the base model and SFT warmup across all six evaluation benchmarks (<a href="https://huggingface.co/datasets/HuggingFaceM4/VQAv2" rel="external nofollow noopener" target="_blank">VQAv2</a>, <a href="https://cs.stanford.edu/people/dorarad/gqa/about.html" rel="external nofollow noopener" target="_blank">GQA</a>, <a href="https://huggingface.co/datasets/lmms-lab/POPE" rel="external nofollow noopener" target="_blank">POPE</a>, <a href="https://scienceqa.github.io/" rel="external nofollow noopener" target="_blank">ScienceQA</a>, <a href="https://textvqa.org/" rel="external nofollow noopener" target="_blank">TextVQA</a>, <a href="https://vizwiz.org/tasks-and-datasets/vqa/" rel="external nofollow noopener" target="_blank">VizWiz</a>)</li> <li>Performance gains saturated beyond a certain training data size, suggesting data quality matters more than quantity at scale</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/syrielle_scale_up-480.webp 480w,/assets/img/projects/syrielle_scale_up-800.webp 800w,/assets/img/projects/syrielle_scale_up-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/syrielle_scale_up.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Scaling results" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> GRPO training progress across six benchmark datasets, showing consistent improvements though gains plateau as more data is added. </div> <h2 id="references">References</h2> <ol> <li> <p><strong>GRPO</strong>: Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo (2024). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. <em>arXiv preprint arXiv:2402.03300</em>. <a href="https://arxiv.org/abs/2402.03300" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2402.03300</a></p> </li> <li> <p><strong>Chameleon</strong>: Chameleon Team (2025). Chameleon: Mixed-Modal Early-Fusion Foundation Models. <em>arXiv preprint arXiv:2405.09818</em>. <a href="https://arxiv.org/abs/2405.09818" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2405.09818</a></p> </li> <li> <p><strong>LoRA</strong>: Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen (2021). LoRA: Low-Rank Adaptation of Large Language Models. <em>arXiv preprint arXiv:2106.09685</em>. <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2106.09685</a></p> </li> </ol> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jakhongir Saydaliev. Last updated: October 26, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?880f023ae5a3457786af14022ff676f0"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>