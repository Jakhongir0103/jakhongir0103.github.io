<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Visual Reasoning | Jakhongir Saydaliev </title> <meta name="author" content="Jakhongir Saydaliev"> <meta name="description" content="Explored GRPO to enhance visual question answering in vision-language models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile.png?9eec2f327f2e2cf3da5d0adcec110cea"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jakhongir0103.github.io/projects/5_project/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jakhongir</span> Saydaliev </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">cv </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Visual Reasoning</h1> <p class="post-description">Explored GRPO to enhance visual question answering in vision-language models</p> </header> <article> <div class="links" style="margin-bottom: 2rem;"> <a href="https://github.com/Jakhongir0103/VLM-R1/blob/main/pdf/Visual_Intelligence_Tech_Report.pdf" class="btn btn-primary btn-sm" role="button" target="_blank" style="background-color: white !important; border: 1px solid black !important; color: black !important; padding: 8px 16px; border-radius: 4px; text-decoration: none; display: inline-block; margin-right: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" rel="external nofollow noopener"> <i class="fas fa-file-pdf"></i> Technical Report </a> <a href="https://github.com/Jakhongir0103/VLM-R1" class="btn btn-primary btn-sm" role="button" target="_blank" style="background-color: white !important; border: 1px solid black !important; color: black !important; padding: 8px 16px; border-radius: 4px; text-decoration: none; display: inline-block; margin-right: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" rel="external nofollow noopener"> <i class="fab fa-github"></i> Code </a> <a href="https://huggingface.co/collections/Jakh0103/visual-intelligence-68398719ee0d35e8b553b5c9" class="btn btn-primary btn-sm" role="button" target="_blank" style="background-color: white !important; border: 1px solid black !important; color: black !important; padding: 8px 16px; border-radius: 4px; text-decoration: none; display: inline-block; margin-right: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" rel="external nofollow noopener"> <i class="fas fa-cube"></i> Checkpoints </a> </div> <p>We investigate how Group Relative Policy Optimization (GRPO), a reinforcement learning technique, can enhance visual reasoning capabilities in vision-language models (VLMs). Our study examines five key dimensions: the alignment between reasoning chains and final answers, grounding reasoning with bounding boxes, generalization from synthetic to real-world data, bias mitigation, and prompt-based reasoning induction. Our findings show that GRPO improves performance and generalization on out-of-domain datasets when structured rewards are used, but reasoning alignment remains imperfect and prompt tuning proves challenging. These results highlight both the potential and current limitations of reinforcement learning for advancing visual reasoning in VLMs.</p> <h2 id="methods">Methods</h2> <p>Our work approach tackles five distinct research questions through targeted experiments:</p> <p><strong>Reasoning-Answer Alignment.</strong> We quantified misalignment between model reasoning and final answers using an LLM-as-judge protocol with GPT-4 mini, assessing whether reasoning traces logically support predicted answers.</p> <p><strong>Grounded Reasoning.</strong> As illustrated in the methodology overview below, we employed a two-stage training approach: first, supervised fine-tuning (SFT) to teach the model to generate bounding boxes within reasoning chains, followed by GRPO$^{[1]}$ training with three reward functions (accuracy, format consistency, and IoU scores).</p> <div class="row justify-content-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/vi_grounding_overview-480.webp 480w,/assets/img/projects/vi_grounding_overview-800.webp 800w,/assets/img/projects/vi_grounding_overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/vi_grounding_overview.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Grounded reasoning methodology" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mt-2"> Two-stage training approach for grounded reasoning: SFT warmup followed by GRPO with structured rewards. </div> <p><strong>Synthetic-to-Real Generalization.</strong> We trained models on the synthetic <a href="https://github.com/princeton-vl/Rel3D" rel="external nofollow noopener" target="_blank">Rel3D</a> dataset and evaluated on both Rel3D and the real-world <a href="https://github.com/princeton-vl/SpatialSense" rel="external nofollow noopener" target="_blank">SpatialSense</a> dataset to assess transfer learning capabilities. We also experimented with augmented inputs including depth images and bounding boxes.</p> <div class="row justify-content-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/vi_Rel3Dexample-480.webp 480w,/assets/img/projects/vi_Rel3Dexample-800.webp 800w,/assets/img/projects/vi_Rel3Dexample-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/vi_Rel3Dexample.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Rel3D example" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/vi_SpatialSenseexample-480.webp 480w,/assets/img/projects/vi_SpatialSenseexample-800.webp 800w,/assets/img/projects/vi_SpatialSenseexample-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/vi_SpatialSenseexample.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SpatialSense example" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mt-2"> <b>Left</b>: Rel3D synthetic dataset features minimally contrastive 3D rendered pairs. <b>Right</b>: SpatialSense real-world dataset shows natural photography. </div> <p><strong>Bias Mitigation.</strong> We created increasingly biased variants of the <a href="https://github.com/cambridgeltl/visual-spatial-reasoning" rel="external nofollow noopener" target="_blank">Visual Spatial Reasoning</a> (VSR) dataset through targeted undersampling and trained models using both SFT and GRPO to measure their robustness to spurious correlations.</p> <p><strong>Prompt Engineering.</strong> We applied soft prompt tuning using the <a href="https://huggingface.co/docs/peft/index" rel="external nofollow noopener" target="_blank">PEFT</a> library, optimizing learnable token prefixes while keeping base model weights frozen, comparing answer-only and reasoning-chain fine-tuning strategies.</p> <h2 id="results">Results</h2> <h4 id="reasoning-answer-alignment">Reasoning-Answer Alignment</h4> <p>Our alignment analysis reveals a concerning trade-off: while GRPO improves task accuracy, it paradoxically decreases reasoning-answer alignment.</p> <table data-toggle="table" class="table table-bordered table-hover text-center align-middle"> <thead> <tr> <th>Model</th> <th>Reasoning</th> <th>Accuracy</th> <th>Alignment</th> </tr> </thead> <tbody> <tr> <td>Qwen-Instruct</td> <td>✓</td> <td>70.59%</td> <td><b>88.29%</b></td> </tr> <tr> <td>Qwen-SFT</td> <td>✗</td> <td>84.12%</td> <td>—</td> </tr> <tr> <td>Qwen-GRPO</td> <td>✓</td> <td><b>86.47%</b></td> <td>82.86%</td> </tr> </tbody> </table> <p></p> <p>GRPO training achieves higher accuracy (86.47% vs 84.12%) but reduces alignment by approximately 6%, suggesting that detailed reasoning traces may reflect pattern-matching rather than genuine logical inference.</p> <h4 id="grounded-reasoning-performance">Grounded Reasoning Performance</h4> <p>Our two-stage grounding approach demonstrates strong improvements, particularly on out-of-domain data:</p> <table data-toggle="table" class="table table-bordered table-hover text-center align-middle"> <thead> <tr> <th>Methods</th> <th>Accuracy Reward</th> <th>Format Reward</th> <th>IoU Reward</th> <th>DrivingVQA (OOD)</th> <th>A-OKVQA (In-Domain)</th> </tr> </thead> <tbody> <tr> <td>SFT-1</td> <td>—</td> <td>—</td> <td>—</td> <td>54.47</td> <td>88.03</td> </tr> <tr> <td>SFT-10</td> <td>—</td> <td>—</td> <td>—</td> <td>51.91</td> <td>85.36</td> </tr> <tr> <td>GRPO</td> <td>✓</td> <td>✓</td> <td>✗</td> <td>57.89</td> <td><b>88.56</b></td> </tr> <tr> <td>GRPO</td> <td>✓</td> <td>✗</td> <td>✓</td> <td><b>61.31</b></td> <td>88.3</td> </tr> <tr> <td>GRPO</td> <td>✓</td> <td>✓</td> <td>✓</td> <td><b>61.31</b></td> <td>88.3</td> </tr> </tbody> </table> <p></p> <p>The most significant gains appear on the out-of-domain <a href="https://huggingface.co/datasets/EPFL-DrivingVQA/DrivingVQA" rel="external nofollow noopener" target="_blank">DrivingVQA</a> dataset, where GRPO with IoU rewards achieves a 12% improvement over the SFT baseline. The bounding box-based reward proves particularly valuable for generalization.</p> <h4 id="synthetic-to-real-generalization">Synthetic-to-Real Generalization</h4> <p>Training on synthetic Rel3D data did not transfer effectively to real-world tasks:</p> <table data-toggle="table" class="table table-bordered table-hover text-center align-middle"> <thead> <tr> <th>Methods</th> <th>Training Data</th> <th>Augmented</th> <th>Rel3D</th> <th>SpatialSense</th> </tr> </thead> <tbody> <tr> <td>SFT-2</td> <td>Rel3D</td> <td>✗</td> <td>53.6%</td> <td>50.8%</td> </tr> <tr> <td>SFT-50</td> <td>Rel3D</td> <td>✗</td> <td><b>55.4%</b></td> <td>46.8%</td> </tr> <tr> <td>GRPO</td> <td>Rel3D</td> <td>✗</td> <td>50.9%</td> <td>48.2%</td> </tr> <tr> <td>GRPO-AUG</td> <td>Rel3D</td> <td>✓</td> <td>48.3%</td> <td>—</td> </tr> <tr> <td>SFT-SS</td> <td>SpatialSense</td> <td>✗</td> <td>37.7%</td> <td><b>76.5%</b></td> </tr> </tbody> </table> <p></p> <p>Surprisingly, GRPO underperformed SFT on this task, with performance near random chance (50%). Adding depth images and bounding boxes as augmented modalities provided no benefit. The stark difference between performance on SpatialSense (76.5%) versus Rel3D (37.7%) when trained on the respective datasets suggests a substantial domain gap between synthetic and real imagery.</p> <h4 id="bias-mitigation">Bias Mitigation</h4> <p>VLMs demonstrated unexpected robustness to dataset-induced bias:</p> <table data-toggle="table" class="table table-bordered table-hover text-center align-middle"> <thead> <tr> <th>Train Data</th> <th>Qwen-SFT</th> <th>Qwen-GRPO</th> </tr> </thead> <tbody> <tr> <td>VSR</td> <td>82.0</td> <td><b>84.8</b></td> </tr> <tr> <td>Biased VSR</td> <td><b>84.6</b></td> <td>82.3</td> </tr> <tr> <td>Strongly Biased VSR</td> <td>79.9</td> <td><b>80.7</b></td> </tr> </tbody> </table> <p></p> <p>Even when introducing extreme textual bias (achieving 100% accuracy on a text-only classifier), model performance remained largely stable. GRPO provided no significant advantage over SFT in mitigating bias. These results suggest that VLMs’ pre-training and instruction tuning make them inherently robust to spurious correlations.</p> <h4 id="prompt-engineering">Prompt Engineering</h4> <p>Soft prompt tuning proved ineffective for inducing reasoning behavior. With only 5 soft prompt tokens and 4 training epochs, the model failed to follow required output formats (0% accuracy under strict evaluation). While training loss decreased significantly when learning from GRPO-generated reasoning traces, no accuracy improvements materialized at test time, likely because the GRPO-generated traces themselves exhibit poor reasoning-answer alignment.</p> <h2 id="references">References</h2> <ol> <li> <strong>GRPO</strong>: Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo (2024). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. <em>arXiv preprint arXiv:2402.03300</em>. <a href="https://arxiv.org/abs/2402.03300" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2402.03300</a> </li> </ol> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jakhongir Saydaliev. Last updated: October 25, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?880f023ae5a3457786af14022ff676f0"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>