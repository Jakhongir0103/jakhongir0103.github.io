<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Mountain Car | Jakhongir Saydaliev </title> <meta name="author" content="Jakhongir Saydaliev"> <meta name="description" content="Handling sparse reward challenges in reinforcement learning using DQN and Dyna-Q algorithms"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile.png?9eec2f327f2e2cf3da5d0adcec110cea"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jakhongir0103.github.io/projects/6_project/"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jakhongir</span> Saydaliev </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">cv </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Mountain Car</h1> <p class="post-description">Handling sparse reward challenges in reinforcement learning using DQN and Dyna-Q algorithms</p> </header> <article> <div class="links" style="margin-bottom: 2rem;"> <a href="https://github.com/Jakhongir0103/mountain-car-reinforcement-learning/blob/main/pdf/report.pdf" class="btn btn-primary btn-sm" role="button" target="_blank" style="background-color: white !important; border: 1px solid black !important; color: black !important; padding: 8px 16px; border-radius: 4px; text-decoration: none; display: inline-block; margin-right: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" rel="external nofollow noopener"> <i class="fas fa-file-pdf"></i> Technical Report </a> <a href="https://github.com/Jakhongir0103/mountain-car-reinforcement-learning" class="btn btn-primary btn-sm" role="button" target="_blank" style="background-color: white !important; border: 1px solid black !important; color: black !important; padding: 8px 16px; border-radius: 4px; text-decoration: none; display: inline-block; margin-right: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" rel="external nofollow noopener"> <i class="fab fa-github"></i> Code </a> </div> <p>The <a href="https://gymnasium.farama.org/environments/classic_control/mountain_car/" rel="external nofollow noopener" target="_blank">Mountain Car</a> environment presents a classic reinforcement learning challenge where an agent must learn to drive a car up a steep hill by building momentum through strategic back-and-forth movements. The sparse reward structure (only -1 per timestep with no intermediate feedback) makes this seemingly simple task surprisingly difficult for standard RL algorithms. We explored both model-free (DQN with auxiliary rewards) and model-based (Dyna-Q) approaches to overcome this challenge.</p> <div class="row justify-content-center"> <div class="col-10 col-md-8 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_2.1-480.webp 480w,/assets/img/projects/mountain_car_2.1-800.webp 800w,/assets/img/projects/mountain_car_2.1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_2.1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Mountain Car Environment" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-10 col-md-8 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_2.2-480.webp 480w,/assets/img/projects/mountain_car_2.2-800.webp 800w,/assets/img/projects/mountain_car_2.2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_2.2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Random Agent Performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mt-2"> <b>Top</b>: The Mountain Car environment with the car starting at the bottom of the valley. <b>Bottom</b>: Episode duration when playing randomly - the agent never succeeds and always hits the 200 timestep limit. </div> <h2 id="methods">Methods</h2> <h4 id="deep-q-learning-dqn">Deep Q-Learning (DQN)</h4> <p>We implemented the standard DQN$^{[1]}$ algorithm with experience replay and target networks. The Q-learning update rule with neural networks is:</p> \[Q_{\theta}(s, a) \leftarrow Q_{\theta}(s, a) + \alpha \left[ r + \gamma \max_{a'} Q_{\hat{\theta}}(s', a') - Q_{\theta}(s, a) \right]\] <p>However, vanilla DQN struggled with the sparse rewards, failing to complete the task even after 1000 episodes despite the loss function converging.</p> <h4 id="auxiliary-reward-functions">Auxiliary Reward Functions</h4> <p>To address the sparse reward problem, we experimented with two approaches:</p> <p><strong>1. Heuristic Reward Function:</strong> We designed a domain-specific reward that incentivizes both position and velocity:</p> \[r_{aux} = |s'_p - s_{p_0}| + \frac{|s'_v|}{2 \times s'_{v_{max}}}\] <p><strong>2. Random Network Distillation (RND):</strong> An environment-agnostic approach that encourages exploration by using the prediction error between a fixed random network and a learned predictor network as intrinsic reward.</p> <h4 id="dyna-q">Dyna-Q</h4> <p>We also implemented Dyna-Q$^{[2]}$, which combines model-free and model-based learning by using a learned environment model to generate simulated experiences. Since Mountain Car has continuous states, we discretized the state space using different bin sizes (small, medium, large) to study the effect of state resolution on learning.</p> <h2 id="results">Results</h2> <p>We report the episode duration over training for each method below. Lower value means the model has learned to finish the task faster.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_episode_vanilla-480.webp 480w,/assets/img/projects/mountain_car_episode_vanilla-800.webp 800w,/assets/img/projects/mountain_car_episode_vanilla-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_episode_vanilla.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_episode_heuristic-480.webp 480w,/assets/img/projects/mountain_car_episode_heuristic-800.webp 800w,/assets/img/projects/mountain_car_episode_heuristic-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_episode_heuristic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_episode_rnd-480.webp 480w,/assets/img/projects/mountain_car_episode_rnd-800.webp 800w,/assets/img/projects/mountain_car_episode_rnd-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_episode_rnd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_episode_dyna-480.webp 480w,/assets/img/projects/mountain_car_episode_dyna-800.webp 800w,/assets/img/projects/mountain_car_episode_dyna-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_episode_dyna.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> </swiper-container> <div class="caption"> Episode duration over training for different algorithms for: (1) Vanilla DQN, (2) DQN with heuristic reward, (3) DQN with RND, (4) Dyna-Q. Lower duration indicates successful task completion. Both auxiliary reward methods help DQN learn, with RND achieving success slightly earlier (~500 episodes) than the heuristic reward (~600 episodes). </div> <h3 id="reward-distribution-analysis">Reward Distribution Analysis</h3> <p>One fascinating insight comes from visualizing where each algorithm accumulates rewards in the state space. For each of the 4 methods, we report the sum of reward per position and velocity. Position=0.5 is the final state.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_3.2-reward%20distribution-480.webp 480w,/assets/img/projects/mountain_car_3.2-reward%20distribution-800.webp 800w,/assets/img/projects/mountain_car_3.2-reward%20distribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_3.2-reward%20distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_3.3-reward%20distribution-480.webp 480w,/assets/img/projects/mountain_car_3.3-reward%20distribution-800.webp 800w,/assets/img/projects/mountain_car_3.3-reward%20distribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_3.3-reward%20distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_3.4-reward%20distribution-480.webp 480w,/assets/img/projects/mountain_car_3.4-reward%20distribution-800.webp 800w,/assets/img/projects/mountain_car_3.4-reward%20distribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_3.4-reward%20distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_4-q_heat_map-480.webp 480w,/assets/img/projects/mountain_car_4-q_heat_map-800.webp 800w,/assets/img/projects/mountain_car_4-q_heat_map-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_4-q_heat_map.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> </swiper-container> <div class="caption"> Heatmaps showing the sum of rewards at each state (position vs velocity) from the last 10k experiences. The heuristic reward creates a clear gradient leading to the goal, while RND explores more broadly across the state space. </div> <h2 id="key-findings">Key Findings</h2> <ol> <li> <p><strong>Sparse rewards are challenging</strong>: Vanilla DQN completely fails without auxiliary rewards, highlighting the importance of reward shaping or intrinsic motivation in sparse reward environments.</p> </li> <li> <p><strong>RND vs Heuristic rewards</strong>: While both approaches succeed, RND learns slightly faster and is more generalizable since it doesn’t require domain knowledge. The heuristic reward creates more interpretable learning patterns focused on reaching the goal.</p> </li> <li> <p><strong>Discretization matters</strong>: For Dyna-Q, medium-sized bins provided the best balance between state resolution and learning speed. Too large bins lose important dynamics, while too small bins slow learning.</p> </li> <li> <p><strong>Multiple policies emerge</strong>: Interestingly, all successful agents learned to complete the task in approximately two distinct durations (~90 or ~150 steps), suggesting multiple valid strategies for solving the Mountain Car problem.</p> </li> </ol> <p>Below is how the model learns to achieve the task during traning at the episodes 99, 499, and 2998:</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_dyna_demo_1-480.webp 480w,/assets/img/projects/mountain_car_dyna_demo_1-800.webp 800w,/assets/img/projects/mountain_car_dyna_demo_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_dyna_demo_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_dyna_demo_1-480.webp 480w,/assets/img/projects/mountain_car_dyna_demo_1-800.webp 800w,/assets/img/projects/mountain_car_dyna_demo_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_dyna_demo_1.png" class="img-fluid rounded z-depth-2" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/mountain_car_dyna_demo_1-480.webp 480w,/assets/img/projects/mountain_car_dyna_demo_1-800.webp 800w,/assets/img/projects/mountain_car_dyna_demo_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/mountain_car_dyna_demo_1.png" class="img-fluid rounded z-depth-3" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </swiper-slide> </swiper-container> <div class="caption"> Evolution of Dyna-Q's exploration over training, showing how the agent progressively discovers more of the state space and refines its policy. </div> <h2 id="references">References</h2> <ol> <li> <p><strong>Deep Q-Learning</strong>: Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller (2013). Playing Atari with Deep Reinforcement Learning. <em>arXiv preprint arXiv:1312.5602</em>. <a href="https://arxiv.org/abs/1312.5602" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/1312.5602</a></p> </li> <li> <p><strong>Dyna-Q</strong>: Baolin Peng and Xiujun Li and Jianfeng Gao and Jingjing Liu and Kam-Fai Wong and Shang-Yu Su (2018). Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning. <em>arXiv preprint arXiv:1801.06176</em>. <a href="https://arxiv.org/abs/1801.06176" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/1801.06176</a></p> </li> </ol> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jakhongir Saydaliev. Last updated: October 11, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?6fd95a99d17166fe284429a5dac84f73"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>