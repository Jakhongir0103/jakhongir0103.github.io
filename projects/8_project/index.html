<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Stance Detection | Jakhongir Saydaliev </title> <meta name="author" content="Jakhongir Saydaliev"> <meta name="description" content="Fine-tuning Large Language Models for argument stance detection in unseen domains"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile.png?9eec2f327f2e2cf3da5d0adcec110cea"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jakhongir0103.github.io/projects/8_project/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jakhongir</span> Saydaliev </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">cv </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Stance Detection</h1> <p class="post-description">Fine-tuning Large Language Models for argument stance detection in unseen domains</p> </header> <article> <div class="links" style="margin-bottom: 2rem;"> <a href="https://github.com/Jakhongir0103/Machine-Learning_EPFL/blob/master/projects/project2/project2_report.pdf" class="btn btn-primary btn-sm" role="button" target="_blank" style="background-color: white !important; border: 1px solid black !important; color: black !important; padding: 8px 16px; border-radius: 4px; text-decoration: none; display: inline-block; margin-right: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" rel="external nofollow noopener"> <i class="fas fa-file-pdf"></i> Technical Report </a> </div> <p><em>Stance detection</em> is the task of determining whether an argument is in favor of, against, or neutral towards a given topic. This has significant applications in social media analysis, misinformation detection, and political discourse understanding. Our work explores how well Large Language Models (LLMs) can be fine-tuned for this task and, importantly, how well they generalize to unseen datasets.</p> <p>This project is part of <a href="https://www.media-initiative.ch/project/commpass/" rel="external nofollow noopener" target="_blank">CommPass</a>, a larger initiative aimed at creating awareness about media polarity by providing readers with visualizations showing where content sits in the “space” of media events like the Russia-Ukraine war or COVID-19.</p> <h2 id="methods">Methods</h2> <h4 id="models-and-fine-tuning-approach">Models and Fine-Tuning Approach</h4> <p>We experimented with three LLMs:</p> <ul> <li> <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" rel="external nofollow noopener" target="_blank">Mistral-7B</a> - 7 billion parameters with advanced features like Grouped Query Attention</li> <li> <a href="https://huggingface.co/meta-llama/Llama-2-7b" rel="external nofollow noopener" target="_blank">Llama-2-7B</a> - 7 billion parameters</li> <li> <a href="https://huggingface.co/microsoft/phi-1_5" rel="external nofollow noopener" target="_blank">Phi-1.5</a> - A smaller 1.3 billion parameter model trained primarily on textbook data</li> </ul> <p>Rather than fine-tuning all parameters (which would be computationally expensive), we used Low-Rank Adaptation (LoRA$^{[1]}$) - a parameter-efficient technique that inserts trainable rank decomposition matrices into selected layers while freezing the pre-trained weights. This dramatically reduces the number of trainable parameters while maintaining performance.</p> <h4 id="datasets">Datasets</h4> <p>We trained and evaluated on two distinct datasets to test generalization:</p> <ol> <li> <p><a href="https://www.saifmohammad.com/WebPages/StanceDataset.htm" rel="external nofollow noopener" target="_blank">SemEval2016</a> - Twitter data focusing on six targets (Abortion, Atheism, Climate Change, Feminist Movement, Hillary Clinton, Donald Trump) with three labels: Favor, Neutral, Against</p> </li> <li> <p><a href="https://research.ibm.com/haifa/dept/vst/debating_data.shtml" rel="external nofollow noopener" target="_blank">IBM-Debater</a> - Claims and evidence from Wikipedia articles covering 33 controversial topics, with only two labels: PRO and CON</p> </li> </ol> <p>A key difference: SemEval uses short targets (1-2 words) while IBM-Debater uses complete sentences.</p> <h2 id="experiments">Experiments</h2> <h4 id="finding-the-right-lora-rank">Finding the Right LoRA Rank</h4> <p>We tested LoRA ranks from 1 to 64 on the full SemEval dataset. The results showed that Mistral consistently outperformed Llama and Phi, but interestingly, there was no clear trend with rank size - lower ranks performed just as well as higher ones.</p> <div class="row"> <div class="col-sm mt-8 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/ml2_rank_experiments.pdf" sizes="95vw"></source> <img src="/assets/img/projects/ml2_rank_experiments.pdf" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="LoRA rank comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Assessing the role of LoRA rank: no significant differences between lower and higher ranks across models. </div> <h4 id="low-data-regimes">Low-Data Regimes</h4> <p>We tested how well models perform when fine-tuned on limited data (1%, 10%, and 50% of the training set). Mistral again proved superior, especially in low-data scenarios. We found that rank choice depends on data volume - rank 1 works better with less data, while rank 8 improves with more data (likely because higher ranks overfit small datasets).</p> <div class="row"> <div class="col-sm mt-8 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/ml2_train_percentage_models.pdf" sizes="95vw"></source> <img src="/assets/img/projects/ml2_train_percentage_models.pdf" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Data regime experiments" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fine-tuning LLMs in different data regimes shows Mistral's robustness even with limited training data. </div> <h2 id="main-results">Main Results</h2> <p>Our best model - Mistral with LoRA rank 16, trained on 70% of both SemEval and IBM-Debater datasets - significantly outperformed all baselines:</p> <div class="row"> <div class="col-sm mt-10 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/ml2_selected_models_f1.pdf" sizes="95vw"></source> <img src="/assets/img/projects/ml2_selected_models_f1.pdf" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="F1 scores across training configurations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> F1-scores of Mistral trained on different dataset combinations. Models with rank 1 used 10% of training data, while rank 8 and 16 used 70%. </div> <h4 id="performance-table">Performance Table</h4> <table data-toggle="table" class="table table-bordered table-hover text-center align-middle"> <thead> <tr> <th>Model</th> <th>Abortion</th> <th>Atheism</th> <th>Climate Change</th> <th>Feminist Movement</th> <th>Hillary Clinton</th> <th>SemEval (avg)</th> <th>IBM (avg)</th> </tr> </thead> <tbody> <tr> <td>BERTweet (baseline)</td> <td>0.65</td> <td>0.76</td> <td>0.79</td> <td>0.65</td> <td>0.69</td> <td>0.70</td> <td>-</td> </tr> <tr> <td>RoBERTa (baseline)</td> <td>0.54</td> <td><b>0.79</b></td> <td>0.80</td> <td>0.64</td> <td>0.71</td> <td>0.68</td> <td>-</td> </tr> <tr> <td>StanceBERTa (baseline)</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>0.61</td> </tr> <tr> <td>Mistral Zero-shot</td> <td>0.54</td> <td>0.33</td> <td>0.55</td> <td>0.57</td> <td>0.66</td> <td>0.54</td> <td>0.44</td> </tr> <tr> <td><b>Mistral Fine-tuned (Ours)</b></td> <td><b>0.71</b></td> <td>0.73</td> <td><b>0.84</b></td> <td><b>0.76</b></td> <td><b>0.80</b></td> <td><b>0.76</b></td> <td><b>0.92</b></td> </tr> </tbody> </table> <p></p> <h4 id="surprising-findings">Surprising Findings</h4> <ol> <li> <p><strong>Cross-dataset generalization</strong>: Models fine-tuned on SemEval alone generalized remarkably well to IBM-Debater, outperforming the baseline despite never seeing that data format during training.</p> </li> <li> <p><strong>Training on both datasets improved neutral class recall</strong> on SemEval, even though IBM-Debater has no neutral labels - suggesting the model learned more nuanced representations.</p> </li> <li> <p><strong>Fine-tuning on SemEval and extrapolating to IBM might lead to better results</strong> than directly fine-tuning on IBM alone.</p> </li> </ol> <div class="row"> <div class="col-sm mt-10 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/ml2_confusion_matrix_r16_semeval-480.webp 480w,/assets/img/projects/ml2_confusion_matrix_r16_semeval-800.webp 800w,/assets/img/projects/ml2_confusion_matrix_r16_semeval-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/ml2_confusion_matrix_r16_semeval.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Confusion matrices" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Confusion matrices showing how training dataset combinations affect classification performance on SemEval test set. </div> <h2 id="references">References</h2> <ol> <li> <strong>LoRA</strong>: Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen (2021). LoRA: Low-Rank Adaptation of Large Language Models. <em>arXiv preprint arXiv:2106.09685</em>. <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2106.09685</a> </li> </ol> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jakhongir Saydaliev. Last updated: October 12, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?880f023ae5a3457786af14022ff676f0"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>